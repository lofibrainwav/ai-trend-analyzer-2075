#!/usr/bin/env python3
"""
ğŸš€ AI Trend Analyzer & YouTube Summarizer
2075ë…„ ë ˆì˜¤ë‚˜ë¥´ë„ ë‹¤ ë¹ˆì¹˜ ìŠ¤íƒ€ì¼ ë°ì´í„° ë¶„ì„ê¸°

ê¸°ëŠ¥:
- YouTube AI íŠ¸ë Œë“œ ì˜ìƒ ìˆ˜ì§‘
- ìë§‰ ì¶”ì¶œ ë° ìš”ì•½
- 20+ ì†ŒìŠ¤ ë¦¬ì„œì¹˜
- Playwright ìŠ¤í…”ìŠ¤ í¬ë¡¤ë§ (0.3ì´ˆ ê°„ê²©)
- ê³ ê¸‰ íŠ¸ë Œë“œ ë¶„ì„
"""

import asyncio
import json
import time
import random
from datetime import datetime, timedelta
import mysql.connector
from typing import Dict, List, Any
import requests
import os

class AITrendAnalyzer:
    def __init__(self):
        self.db_config = {
            'host': 'localhost',
            'port': 3306,
            'user': 'root',
            'password': '',
            'database': 'ai_trends_db'
        }
        
        # ë¦¬ì„œì¹˜ ì†ŒìŠ¤ URLë“¤ (20+ ì‚¬ì´íŠ¸)
        self.research_sources = [
            'https://www.technologyreview.com',
            'https://venturebeat.com/ai/',
            'https://www.theverge.com/ai-artificial-intelligence',
            'https://techcrunch.com/category/artificial-intelligence/',
            'https://www.wired.com/tag/artificial-intelligence/',
            'https://www.forbes.com/ai/',
            'https://www.reuters.com/technology/artificial-intelligence/',
            'https://www.zdnet.com/topic/artificial-intelligence/',
            'https://arstechnica.com/tag/ai/',
            'https://www.engadget.com/tag/ai/',
            'https://www.cnet.com/tech/services-and-software/ai/',
            'https://www.bloomberg.com/technology',
            'https://www.fastcompany.com/section/ai-machine-learning',
            'https://www.axios.com/technology',
            'https://www.businessinsider.com/sai',
            'https://analyticsindiamag.com',
            'https://www.kdnuggets.com',
            'https://towardsdatascience.com',
            'https://machinelearningmastery.com',
            'https://www.datasciencecentral.com',
            'https://www.analyticsvidhya.com',
            'https://www.tensorflow.org/blog',
            'https://ai.googleblog.com',
            'https://openai.com/blog',
            'https://www.anthropic.com/news'
        ]
        
        self.youtube_keywords = [
            'AI trends 2025',
            'artificial intelligence future',
            'machine learning breakthrough',
            'ChatGPT alternatives',
            'AI tools 2025',
            'AGI artificial general intelligence',
            'AI agents',
            'AI automation',
            'AI coding',
            'AI research latest'
        ]
    
    def get_db_connection(self):
        """MySQL ì—°ê²°"""
        return mysql.connector.connect(**self.db_config)
    
    async def search_youtube_videos(self, keyword: str, max_results: int = 10) -> List[Dict]:
        """YouTube ì˜ìƒ ê²€ìƒ‰"""
        print(f"ğŸ¬ YouTube ê²€ìƒ‰: {keyword}")
        
        # ì‹¤ì œ YouTube APIëŠ” ì—¬ê¸°ì„œ í˜¸ì¶œ
        # ì§€ê¸ˆì€ ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        videos = []
        
        # ì„ì‹œë¡œ ëª‡ ê°œ ì˜ìƒ ID ì¶”ê°€
        sample_ids = ['5zuF4Ys1eAw', 'PQgUHLPqIAA', 'xoknlPcv2dA']
        
        for video_id in sample_ids:
            videos.append({
                'video_id': video_id,
                'title': f'AI Trend Video for {keyword}',
                'channel_name': 'AI Channel',
                'view_count': random.randint(10000, 1000000),
                'like_count': random.randint(100, 10000),
                'comment_count': random.randint(10, 1000),
                'published_at': datetime.now() - timedelta(days=random.randint(1, 30))
            })
        
        return videos
    
    async def get_video_transcript(self, video_id: str) -> str:
        """ì˜ìƒ ìë§‰ ì¶”ì¶œ"""
        print(f"ğŸ“ ìë§‰ ì¶”ì¶œ: {video_id}")
        
        # ì‹¤ì œë¡œëŠ” YouTube APIë‚˜ í¬ë¡¤ë§ìœ¼ë¡œ ìë§‰ ì¶”ì¶œ
        # ì§€ê¸ˆì€ ë”ë¯¸ ë°ì´í„°
        return f"This is a sample transcript for video {video_id} about AI trends and developments..."
    
    async def summarize_content(self, content: str, content_type: str = "transcript") -> str:
        """ì½˜í…ì¸  ìš”ì•½ (AI ê¸°ë°˜)"""
        print(f"ğŸ§  ì½˜í…ì¸  ìš”ì•½ ì¤‘... ({content_type})")
        
        # ì‹¤ì œë¡œëŠ” OpenAI APIë‚˜ ë‹¤ë¥¸ LLMìœ¼ë¡œ ìš”ì•½
        # ì§€ê¸ˆì€ ê°„ë‹¨í•œ ìš”ì•½ ì‹œë®¬ë ˆì´ì…˜
        words = content.split()
        if len(words) > 100:
            summary = " ".join(words[:50]) + "... [AI ìš”ì•½]"
        else:
            summary = content
        
        return summary
    
    async def stealth_crawl_with_playwright(self, url: str, clicks: int = 5) -> Dict:
        """Playwright ìŠ¤í…”ìŠ¤ í¬ë¡¤ë§ (0.3ì´ˆ ê°„ê²©)"""
        print(f"ğŸ•·ï¸ ìŠ¤í…”ìŠ¤ í¬ë¡¤ë§: {url}")
        
        # ì‹¤ì œë¡œëŠ” Playwrightë¡œ êµ¬í˜„
        # ì§€ê¸ˆì€ ì‹œë®¬ë ˆì´ì…˜
        await asyncio.sleep(0.3 * clicks)  # í´ë¦­ ê°„ê²© ì‹œë®¬ë ˆì´ì…˜
        
        return {
            'url': url,
            'title': f'Article from {url}',
            'content': f'Sample content from {url} with AI insights...',
            'credibility_score': random.uniform(0.7, 1.0)
        }
    
    async def research_multiple_sources(self, topic: str) -> List[Dict]:
        """20+ ì†ŒìŠ¤ì—ì„œ ë¦¬ì„œì¹˜"""
        print(f"ğŸ” ë‹¤ì¤‘ ì†ŒìŠ¤ ë¦¬ì„œì¹˜: {topic}")
        
        research_results = []
        
        # ê° ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
        tasks = []
        for source in self.research_sources[:20]:  # ì²« 20ê°œ ì†ŒìŠ¤ ì‚¬ìš©
            task = self.stealth_crawl_with_playwright(source, clicks=3)
            tasks.append(task)
        
        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, dict):
                research_results.append(result)
        
        return research_results
    
    async def analyze_trends(self, videos: List[Dict], research_data: List[Dict]) -> Dict:
        """íŠ¸ë Œë“œ ë¶„ì„"""
        print("ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶„ì„
        all_content = ""
        for video in videos:
            all_content += video.get('transcript', '') + " "
        
        for research in research_data:
            all_content += research.get('content', '') + " "
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ì‚¬ìš©)
        ai_keywords = ['AI', 'machine learning', 'ChatGPT', 'AGI', 'automation', 'neural networks']
        keyword_counts = {}
        
        for keyword in ai_keywords:
            count = all_content.lower().count(keyword.lower())
            keyword_counts[keyword] = count
        
        # íŠ¸ë Œë“œ ì ìˆ˜ ê³„ì‚°
        total_engagement = sum(video.get('view_count', 0) for video in videos)
        avg_credibility = sum(research.get('credibility_score', 0) for research in research_data) / len(research_data) if research_data else 0
        
        trend_score = (total_engagement / 100000) * avg_credibility
        
        return {
            'keyword_counts': keyword_counts,
            'trend_score': trend_score,
            'total_sources': len(videos) + len(research_data),
            'analysis_summary': f"Based on {len(videos)} videos and {len(research_data)} research sources, the AI trend analysis shows significant interest in {max(keyword_counts, key=keyword_counts.get)} with a confidence score of {trend_score:.2f}"
        }
    
    async def save_to_database(self, videos: List[Dict], research_data: List[Dict], analysis: Dict):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        print("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")
        
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            # YouTube ì˜ìƒ ë°ì´í„° ì €ì¥
            for video in videos:
                cursor.execute("""
                    INSERT IGNORE INTO youtube_videos 
                    (video_id, title, channel_name, view_count, like_count, comment_count, published_at, transcript, summary, ai_keywords, trend_score)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    video['video_id'],
                    video['title'],
                    video['channel_name'],
                    video['view_count'],
                    video['like_count'],
                    video['comment_count'],
                    video['published_at'],
                    video.get('transcript', ''),
                    video.get('summary', ''),
                    json.dumps(analysis['keyword_counts']),
                    analysis['trend_score']
                ))
            
            # ë¦¬ì„œì¹˜ ë°ì´í„° ì €ì¥
            for research in research_data:
                cursor.execute("""
                    INSERT INTO research_sources 
                    (source_url, source_type, title, content, credibility_score)
                    VALUES (%s, %s, %s, %s, %s)
                """, (
                    research['url'],
                    'web_crawl',
                    research['title'],
                    research['content'],
                    research['credibility_score']
                ))
            
            # íŠ¸ë Œë“œ ë¶„ì„ ì €ì¥
            cursor.execute("""
                INSERT INTO trend_analysis 
                (trend_topic, confidence_score, supporting_sources, analysis_summary)
                VALUES (%s, %s, %s, %s)
            """, (
                'AI Trends 2025',
                analysis['trend_score'],
                json.dumps({'total_sources': analysis['total_sources']}),
                analysis['analysis_summary']
            ))
            
            conn.commit()
            print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
            conn.rollback()
        finally:
            cursor.close()
            conn.close()
    
    async def run_full_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ AI íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘!")
        print("=" * 50)
        
        all_videos = []
        all_research = []
        
        # Step 1: YouTube ì˜ìƒ ìˆ˜ì§‘
        for keyword in self.youtube_keywords[:3]:  # ì²˜ìŒ 3ê°œ í‚¤ì›Œë“œë¡œ í…ŒìŠ¤íŠ¸
            videos = await self.search_youtube_videos(keyword)
            
            # ê° ì˜ìƒì˜ ìë§‰ ì¶”ì¶œ ë° ìš”ì•½
            for video in videos:
                transcript = await self.get_video_transcript(video['video_id'])
                summary = await self.summarize_content(transcript, "transcript")
                
                video['transcript'] = transcript
                video['summary'] = summary
            
            all_videos.extend(videos)
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            await asyncio.sleep(1)
        
        # Step 2: ë‹¤ì¤‘ ì†ŒìŠ¤ ë¦¬ì„œì¹˜
        research_data = await self.research_multiple_sources("AI trends 2025")
        all_research.extend(research_data)
        
        # Step 3: íŠ¸ë Œë“œ ë¶„ì„
        analysis = await self.analyze_trends(all_videos, all_research)
        
        # Step 4: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        await self.save_to_database(all_videos, all_research, analysis)
        
        # Step 5: ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 50)
        print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
        print(f"ğŸ“º ìˆ˜ì§‘ëœ YouTube ì˜ìƒ: {len(all_videos)}ê°œ")
        print(f"ğŸ” ë¦¬ì„œì¹˜ ì†ŒìŠ¤: {len(all_research)}ê°œ")
        print(f"ğŸ“ˆ íŠ¸ë Œë“œ ì ìˆ˜: {analysis['trend_score']:.2f}")
        print(f"ğŸ¯ ì£¼ìš” í‚¤ì›Œë“œ: {max(analysis['keyword_counts'], key=analysis['keyword_counts'].get)}")
        print(f"ğŸ“ ë¶„ì„ ìš”ì•½: {analysis['analysis_summary']}")
        print("=" * 50)
        
        return analysis

# ì‹¤í–‰ í•¨ìˆ˜
async def main():
    analyzer = AITrendAnalyzer()
    await analyzer.run_full_analysis()

if __name__ == "__main__":
    asyncio.run(main())
