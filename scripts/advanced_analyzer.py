        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            # YouTube ì˜ìƒ ë°ì´í„° ì €ì¥
            youtube_insert_query = """
                INSERT IGNORE INTO youtube_videos 
                (video_id, title, channel_name, view_count, like_count, comment_count, 
                 published_at, duration, transcript, summary, ai_keywords, trend_score)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            for video in youtube_data:
                cursor.execute(youtube_insert_query, (
                    video['video_id'],
                    video['title'][:500],  # ê¸¸ì´ ì œí•œ
                    video['channel_name'][:255],
                    video['view_count'],
                    video['like_count'],
                    video['comment_count'],
                    video['published_at'],
                    video.get('duration', ''),
                    video.get('transcript', '')[:10000],  # ê¸¸ì´ ì œí•œ
                    video.get('summary', '')[:2000],
                    json.dumps(video.get('ai_keywords', [])),
                    video.get('trend_score', 0)
                ))
            
            # ë¦¬ì„œì¹˜ ì†ŒìŠ¤ ë°ì´í„° ì €ì¥
            research_insert_query = """
                INSERT INTO research_sources 
                (source_url, source_type, title, content, ai_insights, credibility_score)
                VALUES (%s, %s, %s, %s, %s, %s)
            """
            
            for research in research_data:
                content_text = json.dumps({
                    'headlines': research.get('headlines', [])[:50],  # ìƒìœ„ 50ê°œë§Œ
                    'summaries': research.get('summaries', [])[:20],
                    'click_success_rate': research.get('click_success_rate', 0)
                })
                
                cursor.execute(research_insert_query, (
                    research['source_url'][:500],
                    'stealth_crawl',
                    research['title'][:500],
                    content_text,
                    research.get('ai_insights', '')[:2000],
                    research.get('credibility_score', 0)
                ))
            
            # ê³ ê¸‰ íŠ¸ë Œë“œ ë¶„ì„ ì €ì¥
            trend_insert_query = """
                INSERT INTO trend_analysis 
                (trend_topic, confidence_score, supporting_sources, analysis_summary)
                VALUES (%s, %s, %s, %s)
            """
            
            supporting_sources = {
                'youtube_videos': len(youtube_data),
                'research_sources': len(research_data),
                'top_keywords': analysis['top_keywords'],
                'predictions': analysis['trend_predictions']
            }
            
            cursor.execute(trend_insert_query, (
                'Advanced AI Trends 2025',
                analysis['trend_score'],
                json.dumps(supporting_sources),
                analysis['summary'][:2000]
            ))
            
            conn.commit()
            print("âœ… ëª¨ë“  ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
            
            # ì €ì¥ëœ ë°ì´í„° í†µê³„ ì¶œë ¥
            cursor.execute("SELECT COUNT(*) FROM youtube_videos")
            youtube_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM research_sources")
            research_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM trend_analysis")
            analysis_count = cursor.fetchone()[0]
            
            print(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©:")
            print(f"   YouTube ì˜ìƒ: {youtube_count}ê°œ")
            print(f"   ë¦¬ì„œì¹˜ ì†ŒìŠ¤: {research_count}ê°œ")
            print(f"   íŠ¸ë Œë“œ ë¶„ì„: {analysis_count}ê°œ")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            conn.rollback()
        finally:
            cursor.close()
            conn.close()
    
    async def generate_final_report(self, analysis: Dict) -> str:
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""
ğŸ¤– AI íŠ¸ë Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸ - {datetime.now().strftime('%Y-%m-%d %H:%M')}
{'='*80}

ğŸ“Š ë¶„ì„ ê°œìš”
- ì´ ë¶„ì„ ì†ŒìŠ¤: {analysis['total_sources']}ê°œ
- YouTube ì˜ìƒ: {analysis['youtube_videos']}ê°œ
- ë¦¬ì„œì¹˜ ì†ŒìŠ¤: {analysis['research_sources']}ê°œ
- ì „ì²´ íŠ¸ë Œë“œ ì ìˆ˜: {analysis['trend_score']}/1.0
- ì‹ ë¢°ë„ ì ìˆ˜: {analysis['credibility_score']}/1.0

ğŸ”¥ ìƒìœ„ AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ
"""
        
        for i, (keyword, count) in enumerate(list(analysis['top_keywords'].items())[:10], 1):
            report += f"{i:2d}. {keyword:<30} (ì–¸ê¸‰ {count}íšŒ)\n"
        
        report += f"""
ğŸ¯ íŠ¸ë Œë“œ ì˜ˆì¸¡
"""
        
        for prediction in analysis['trend_predictions']:
            report += f"""
â–¶ {prediction['trend']}
  ì‹ ë¢°ë„: {prediction['confidence']:.1%}
  ì˜í–¥ë„: {prediction['impact_level']}
  ì˜ˆìƒ íƒ€ì„ë¼ì¸: {prediction['timeline']}
  ì˜ˆì¸¡: {prediction['prediction']}
"""
        
        report += f"""
ğŸ“ˆ ë¶„ì„ ê²°ê³¼ ìš”ì•½
{analysis['summary']}

âš¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸
- í˜„ì¬ AI ë¶„ì•¼ì—ì„œ ê°€ì¥ ì£¼ëª©ë°›ëŠ” í‚¤ì›Œë“œëŠ” '{list(analysis['top_keywords'].keys())[0] if analysis['top_keywords'] else 'N/A'}'ì…ë‹ˆë‹¤
- YouTube íŠ¸ë Œë“œ ì ìˆ˜({analysis['youtube_trend_score']:.2f})ì™€ ë¦¬ì„œì¹˜ ì‹ ë¢°ë„({analysis['credibility_score']:.2f})ë¥¼ ì¢…í•©í•œ ê²°ê³¼ì…ë‹ˆë‹¤
- {len(analysis['trend_predictions'])}ê°œì˜ ì£¼ìš” íŠ¸ë Œë“œì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì œê³µí•©ë‹ˆë‹¤

ğŸš€ ê¶Œì¥ ì•¡ì…˜
1. ìƒìœ„ 3ê°œ íŠ¸ë Œë“œ í‚¤ì›Œë“œì— ëŒ€í•œ ì‹¬í™” ì—°êµ¬ ì§„í–‰
2. ë†’ì€ ì‹ ë¢°ë„(>0.8)ë¥¼ ê°€ì§„ ì˜ˆì¸¡ì— ëŒ€í•œ íˆ¬ì ê²€í† 
3. í–¥í›„ 3-6ê°œì›” ë‚´ ê´€ë ¨ ê¸°ìˆ  ë™í–¥ ëª¨ë‹ˆí„°ë§ ê°•í™”

{'='*80}
ë¦¬í¬íŠ¸ ìƒì„± ì‹œê°„: {analysis['analysis_timestamp']}
"""
        
        return report
    
    async def run_comprehensive_analysis(self):
        """ì¢…í•© ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ 2075ë…„ ë ˆì˜¤ë‚˜ë¥´ë„ ë‹¤ ë¹ˆì¹˜ ìŠ¤íƒ€ì¼ AI íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘!")
        print("="*80)
        
        start_time = time.time()
        
        try:
            # Phase 1: YouTube ê³ ê¸‰ ë¦¬ì„œì¹˜
            print("\nğŸ“º Phase 1: YouTube ê³ ê¸‰ ë¶„ì„")
            print("-" * 50)
            youtube_data = await self.advanced_youtube_research()
            
            # Phase 2: 20+ ì†ŒìŠ¤ ëŒ€ê·œëª¨ ë¦¬ì„œì¹˜
            print("\nğŸ” Phase 2: ëŒ€ê·œëª¨ ì†ŒìŠ¤ ë¦¬ì„œì¹˜")
            print("-" * 50)
            research_data = await self.massive_source_research()
            
            # Phase 3: ê³ ê¸‰ íŠ¸ë Œë“œ ë¶„ì„
            print("\nğŸ“Š Phase 3: ê³ ê¸‰ íŠ¸ë Œë“œ ë¶„ì„")
            print("-" * 50)
            analysis = await self.advanced_trend_analysis(youtube_data, research_data)
            
            # Phase 4: ë°ì´í„° ì €ì¥
            print("\nğŸ’¾ Phase 4: ë°ì´í„° ì €ì¥")
            print("-" * 50)
            await self.save_comprehensive_data(youtube_data, research_data, analysis)
            
            # Phase 5: ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
            print("\nğŸ“‹ Phase 5: ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±")
            print("-" * 50)
            final_report = await self.generate_final_report(analysis)
            
            # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
            report_filename = f"/Users/Jadaking/DataLab/projects/ai_trend_analyzer/data/ai_trend_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write(final_report)
            
            # ê²°ê³¼ ì¶œë ¥
            execution_time = time.time() - start_time
            
            print("\n" + "="*80)
            print("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
            print(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ")
            print(f"ğŸ“ ë¦¬í¬íŠ¸ ì €ì¥: {report_filename}")
            print("="*80)
            
            print(final_report)
            
            return {
                'youtube_data': youtube_data,
                'research_data': research_data,
                'analysis': analysis,
                'report_file': report_filename,
                'execution_time': execution_time
            }
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return None

# ì‹¤í–‰ í•¨ìˆ˜
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¤– ê³ ê¸‰ AI íŠ¸ë Œë“œ ë¶„ì„ê¸° ì‹œì‘")
    print("Leonardo da Vinci 2075 Edition")
    print("="*80)
    
    analyzer = AdvancedAITrendAnalyzer()
    result = await analyzer.run_comprehensive_analysis()
    
    if result:
        print("\nğŸ¯ ë¶„ì„ ì„±ê³µ!")
        print(f"ğŸ“Š ìˆ˜ì§‘ëœ YouTube ì˜ìƒ: {len(result['youtube_data'])}ê°œ")
        print(f"ğŸ” ë¦¬ì„œì¹˜ëœ ì†ŒìŠ¤: {len(result['research_data'])}ê°œ")
        print(f"ğŸ“ˆ ìµœì¢… íŠ¸ë Œë“œ ì ìˆ˜: {result['analysis']['trend_score']:.3f}")
        print(f"ğŸ“ ë¦¬í¬íŠ¸ íŒŒì¼: {result['report_file']}")
    else:
        print("âŒ ë¶„ì„ ì‹¤íŒ¨")

if __name__ == "__main__":
    # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
    asyncio.run(main())
