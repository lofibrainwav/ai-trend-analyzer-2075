#!/usr/bin/env python3
"""
ğŸ¯ AI íŠ¸ë Œë“œ ë¶„ì„ê¸° ë°ëª¨
ì‹¤ì œ ì‘ë™ ë°ëª¨ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ë²„ì „
"""

import asyncio
import mysql.connector
import json
import random
from datetime import datetime, timedelta

class AITrendDemo:
    def __init__(self):
        self.db_config = {
            'host': 'localhost',
            'port': 24,
            'user': 'root',
            'password': '',
            'database': 'ai_trends_db'
        }
    
    def get_db_connection(self):
        return mysql.connector.connect(**self.db_config)
    
    async def simulate_youtube_analysis(self):
        """YouTube ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜"""
        print("ğŸ¬ YouTube AI íŠ¸ë Œë“œ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜...")
        
        # ìƒ˜í”Œ YouTube ë°ì´í„°
        sample_videos = [
            {
                'video_id': 'ai_trend_2025_1',
                'title': 'AI 2025ë…„ ì „ë§: ChatGPT vs Claude vs Gemini',
                'channel_name': 'AI Tech Review',
                'view_count': random.randint(100000, 1000000),
                'like_count': random.randint(5000, 50000),
                'comment_count': random.randint(500, 5000),
                'published_at': datetime.now() - timedelta(days=random.randint(1, 30)),
                'transcript': 'AI ê¸°ìˆ ì´ ê¸‰ì†ë„ë¡œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. 2025ë…„ì—ëŠ” ChatGPT, Claude, Gemini ë“± ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë“¤ì´ ë”ìš± ì •êµí•´ì§ˆ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. íŠ¹íˆ ë©€í‹°ëª¨ë‹¬ AIì™€ AI ì—ì´ì „íŠ¸ ê¸°ìˆ ì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.',
                'ai_keywords': ['ChatGPT', 'Claude', 'Gemini', 'LLM', 'multimodal AI', 'AI agents']
            },
            {
                'video_id': 'ai_trend_2025_2',
                'title': '2025 AI í˜ì‹ : ììœ¨ì£¼í–‰ë¶€í„° ì˜ë£Œ AIê¹Œì§€',
                'channel_name': 'Future Tech',
                'view_count': random.randint(150000, 800000),
                'like_count': random.randint(8000, 40000),
                'comment_count': random.randint(800, 4000),
                'published_at': datetime.now() - timedelta(days=random.randint(1, 20)),
                'transcript': 'ììœ¨ì£¼í–‰ ê¸°ìˆ ê³¼ ì˜ë£Œ AIê°€ 2025ë…„ ì£¼ìš” íŠ¸ë Œë“œë¡œ ë¶€ìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì»´í“¨í„° ë¹„ì „ê³¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ë” ì •í™•í•˜ê³  ì•ˆì „í•œ AI ì‹œìŠ¤í…œì´ ê°œë°œë˜ê³  ìˆìŠµë‹ˆë‹¤.',
                'ai_keywords': ['autonomous driving', 'medical AI', 'computer vision', 'deep learning', 'AI safety']
            },
            {
                'video_id': 'ai_trend_2025_3',
                'title': 'AI ì½”ë”© ë„êµ¬ì˜ í˜ì‹ : GitHub Copilotë¶€í„° Claudeê¹Œì§€',
                'channel_name': 'Code With AI',
                'view_count': random.randint(200000, 1200000),
                'like_count': random.randint(12000, 60000),
                'comment_count': random.randint(1200, 6000),
                'published_at': datetime.now() - timedelta(days=random.randint(5, 45)),
                'transcript': 'AI ì½”ë”© ë„êµ¬ë“¤ì´ í”„ë¡œê·¸ë˜ë¨¸ì˜ ìƒì‚°ì„±ì„ í˜ì‹ ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤. GitHub Copilot, Claude, Cursor ë“±ì´ ì½”ë“œ ìƒì„±ê³¼ ë””ë²„ê¹…ì—ì„œ ë†€ë¼ìš´ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.',
                'ai_keywords': ['AI coding', 'GitHub Copilot', 'Claude', 'code generation', 'programming AI']
            },
            {
                'video_id': 'ai_trend_2025_4',
                'title': 'AI ìœ¤ë¦¬ì™€ ì•ˆì „ì„±: 2025ë…„ í•µì‹¬ ê³¼ì œ',
                'channel_name': 'AI Ethics Lab',
                'view_count': random.randint(80000, 500000),
                'like_count': random.randint(4000, 25000),
                'comment_count': random.randint(400, 2500),
                'published_at': datetime.now() - timedelta(days=random.randint(10, 60)),
                'transcript': 'AI ê¸°ìˆ ì´ ë°œì „í• ìˆ˜ë¡ ìœ¤ë¦¬ì™€ ì•ˆì „ì„± ë¬¸ì œê°€ ì¤‘ìš”í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤. AI ì–¼ë¼ì¸ë¨¼íŠ¸, í¸í–¥ì„± ì œê±°, íˆ¬ëª…ì„± í™•ë³´ê°€ 2025ë…„ ì£¼ìš” ê³¼ì œì…ë‹ˆë‹¤.',
                'ai_keywords': ['AI ethics', 'AI safety', 'AI alignment', 'bias', 'transparency']
            },
            {
                'video_id': 'ai_trend_2025_5',
                'title': 'Edge AIì™€ ëª¨ë°”ì¼ AI: ì°¨ì„¸ëŒ€ ì»´í“¨íŒ…',
                'channel_name': 'Mobile Tech Trends',
                'view_count': random.randint(120000, 700000),
                'like_count': random.randint(6000, 35000),
                'comment_count': random.randint(600, 3500),
                'published_at': datetime.now() - timedelta(days=random.randint(3, 25)),
                'transcript': 'Edge AIì™€ ëª¨ë°”ì¼ AI ê¸°ìˆ ì´ ìŠ¤ë§ˆíŠ¸í°ê³¼ IoT ë””ë°”ì´ìŠ¤ì—ì„œ ì‹¤ì‹œê°„ AI ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. NPUì™€ ì „ìš© ì¹©ì…‹ ê°œë°œì´ í™œë°œí•©ë‹ˆë‹¤.',
                'ai_keywords': ['Edge AI', 'mobile AI', 'NPU', 'IoT', 'real-time processing']
            }
        ]
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            for video in sample_videos:
                cursor.execute("""
                    INSERT IGNORE INTO youtube_videos 
                    (video_id, title, channel_name, view_count, like_count, comment_count, 
                     published_at, transcript, ai_keywords, trend_score)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    video['video_id'],
                    video['title'],
                    video['channel_name'],
                    video['view_count'],
                    video['like_count'],
                    video['comment_count'],
                    video['published_at'],
                    video['transcript'],
                    json.dumps(video['ai_keywords']),
                    self.calculate_trend_score(video)
                ))
            
            conn.commit()
            print(f"âœ… YouTube ë¶„ì„ ì™„ë£Œ: {len(sample_videos)}ê°œ ì˜ìƒ ì €ì¥")
            
        except Exception as e:
            print(f"âŒ YouTube ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
            conn.rollback()
        finally:
            cursor.close()
            conn.close()
        
        return sample_videos
    
    def calculate_trend_score(self, video):
        """íŠ¸ë Œë“œ ì ìˆ˜ ê³„ì‚°"""
        view_score = min(1.0, video['view_count'] / 1000000)
        engagement_rate = (video['like_count'] + video['comment_count']) / max(video['view_count'], 1)
        engagement_score = min(1.0, engagement_rate * 100)
        
        days_old = (datetime.now() - video['published_at']).days
        recency_score = max(0.1, 1.0 - (days_old / 90))
        
        return round((view_score * 0.4 + engagement_score * 0.3 + recency_score * 0.3), 3)
    
    async def simulate_research_analysis(self):
        """ë¦¬ì„œì¹˜ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜"""
        print("ğŸ” 20+ ì†ŒìŠ¤ ë¦¬ì„œì¹˜ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜...")
        
        # ìƒ˜í”Œ ë¦¬ì„œì¹˜ ë°ì´í„°
        research_sources = [
            {
                'source_name': 'MIT Technology Review',
                'source_url': 'https://www.technologyreview.com/ai-trends-2025',
                'title': 'The AI Trends That Will Define 2025',
                'content': 'Large language models continue to evolve with better reasoning capabilities. Multimodal AI systems are becoming mainstream.',
                'credibility_score': 0.95,
                'ai_insights': 'Focus on LLM advancement and multimodal systems'
            },
            {
                'source_name': 'Stanford AI Report',
                'source_url': 'https://aiindex.stanford.edu/2025-report',
                'title': 'AI Index 2025: Annual Report',
                'content': 'AI investment reached record highs. Computer vision and NLP showing significant progress.',
                'credibility_score': 0.98,
                'ai_insights': 'Investment trends and technical progress indicators'
            },
            {
                'source_name': 'OpenAI Research',
                'source_url': 'https://openai.com/research/ai-safety-2025',
                'title': 'AI Safety and Alignment Progress',
                'content': 'Constitutional AI and RLHF methods improving AI safety. Alignment research priorities for 2025.',
                'credibility_score': 0.92,
                'ai_insights': 'AI safety methodologies and alignment techniques'
            },
            {
                'source_name': 'Google DeepMind Blog',
                'source_url': 'https://deepmind.com/blog/gemini-ultra-2025',
                'title': 'Gemini Ultra: Next Generation Capabilities',
                'content': 'Advanced reasoning, code generation, and multimodal understanding in latest Gemini models.',
                'credibility_score': 0.90,
                'ai_insights': 'Gemini model capabilities and benchmarks'
            },
            {
                'source_name': 'Anthropic Research',
                'source_url': 'https://anthropic.com/claude-3-opus-analysis',
                'title': 'Claude 3 Opus: Constitutional AI in Practice',
                'content': 'Constitutional AI methods showing promise for safer, more aligned AI systems.',
                'credibility_score': 0.94,
                'ai_insights': 'Constitutional AI and safety-focused development'
            }
        ]
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            for research in research_sources:
                cursor.execute("""
                    INSERT INTO research_sources 
                    (source_url, source_type, title, content, ai_insights, credibility_score)
                    VALUES (%s, %s, %s, %s, %s, %s)
                """, (
                    research['source_url'],
                    'research_simulation',
                    research['title'],
                    research['content'],
                    research['ai_insights'],
                    research['credibility_score']
                ))
            
            conn.commit()
            print(f"âœ… ë¦¬ì„œì¹˜ ë¶„ì„ ì™„ë£Œ: {len(research_sources)}ê°œ ì†ŒìŠ¤ ì €ì¥")
            
        except Exception as e:
            print(f"âŒ ë¦¬ì„œì¹˜ ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")
            conn.rollback()
        finally:
            cursor.close()
            conn.close()
        
        return research_sources
    
    async def generate_trend_analysis(self, youtube_data, research_data):
        """íŠ¸ë Œë“œ ë¶„ì„ ìƒì„±"""
        print("ğŸ“Š ì¢…í•© íŠ¸ë Œë“œ ë¶„ì„ ìƒì„±...")
        
        # ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘
        all_keywords = []
        for video in youtube_data:
            all_keywords.extend(video['ai_keywords'])
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keyword_freq = {}
        for keyword in all_keywords:
            keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1
        
        # ìƒìœ„ í‚¤ì›Œë“œ
        top_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        avg_credibility = sum(r['credibility_score'] for r in research_data) / len(research_data)
        
        # YouTube íŠ¸ë Œë“œ ì ìˆ˜ í‰ê· 
        youtube_scores = [self.calculate_trend_score(video) for video in youtube_data]
        avg_youtube_trend = sum(youtube_scores) / len(youtube_scores)
        
        # ìµœì¢… íŠ¸ë Œë“œ ì ìˆ˜
        final_trend_score = (avg_credibility * 0.6 + avg_youtube_trend * 0.4)
        
        analysis = {
            'top_keywords': dict(top_keywords),
            'trend_score': round(final_trend_score, 3),
            'credibility_score': round(avg_credibility, 3),
            'youtube_trend_score': round(avg_youtube_trend, 3),
            'total_sources': len(youtube_data) + len(research_data),
            'analysis_summary': f"ë¶„ì„ëœ {len(youtube_data)}ê°œ YouTube ì˜ìƒê³¼ {len(research_data)}ê°œ ë¦¬ì„œì¹˜ ì†ŒìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í˜„ì¬ AI íŠ¸ë Œë“œëŠ” '{top_keywords[0][0] if top_keywords else 'N/A'}'ì— ì§‘ì¤‘ë˜ì–´ ìˆìœ¼ë©°, ì „ì²´ íŠ¸ë Œë“œ ì ìˆ˜ëŠ” {final_trend_score:.2f}ì…ë‹ˆë‹¤.",
            'predictions': [
                {
                    'trend': top_keywords[0][0] if top_keywords else 'AI ë°œì „',
                    'confidence': min(0.95, final_trend_score),
                    'timeline': '2025ë…„ í•˜ë°˜ê¸°',
                    'impact': 'High'
                },
                {
                    'trend': 'ë©€í‹°ëª¨ë‹¬ AI',
                    'confidence': 0.87,
                    'timeline': '2025ë…„ ìƒë°˜ê¸°',
                    'impact': 'High'
                },
                {
                    'trend': 'AI ì•ˆì „ì„±',
                    'confidence': 0.79,
                    'timeline': 'ì§€ì†ì ',
                    'impact': 'Medium'
                }
            ]
        }
        
        # íŠ¸ë Œë“œ ë¶„ì„ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                INSERT INTO trend_analysis 
                (trend_topic, confidence_score, supporting_sources, analysis_summary)
                VALUES (%s, %s, %s, %s)
            """, (
                'AI Trends Demo 2025',
                analysis['trend_score'],
                json.dumps({
                    'youtube_videos': len(youtube_data),
                    'research_sources': len(research_data),
                    'top_keywords': analysis['top_keywords']
                }),
                analysis['analysis_summary']
            ))
            
            conn.commit()
            print("âœ… íŠ¸ë Œë“œ ë¶„ì„ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ íŠ¸ë Œë“œ ë¶„ì„ ì €ì¥ ì˜¤ë¥˜: {e}")
            conn.rollback()
        finally:
            cursor.close()
            conn.close()
        
        return analysis
    
    def generate_final_report(self, analysis):
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""
ğŸ¤– AI íŠ¸ë Œë“œ ë¶„ì„ ë°ëª¨ ë¦¬í¬íŠ¸
{'='*60}
ğŸ“… ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“Š ë¶„ì„ ê°œìš”
- ì´ ë¶„ì„ ì†ŒìŠ¤: {analysis['total_sources']}ê°œ
- ì „ì²´ íŠ¸ë Œë“œ ì ìˆ˜: {analysis['trend_score']:.3f}/1.0
- ì‹ ë¢°ë„ ì ìˆ˜: {analysis['credibility_score']:.3f}/1.0
- YouTube íŠ¸ë Œë“œ ì ìˆ˜: {analysis['youtube_trend_score']:.3f}/1.0

ğŸ”¥ ìƒìœ„ AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ
"""
        
        for i, (keyword, count) in enumerate(list(analysis['top_keywords'].items())[:5], 1):
            report += f"{i}. {keyword:<25} ({count}íšŒ ì–¸ê¸‰)\n"
        
        report += f"""
ğŸ¯ íŠ¸ë Œë“œ ì˜ˆì¸¡
"""
        
        for pred in analysis['predictions']:
            report += f"""
â–¶ {pred['trend']}
  ì‹ ë¢°ë„: {pred['confidence']:.1%}
  ì˜ˆìƒ ì‹œê¸°: {pred['timeline']}
  ì˜í–¥ë„: {pred['impact']}
"""
        
        report += f"""
ğŸ“ˆ ë¶„ì„ ê²°ê³¼ ìš”ì•½
{analysis['analysis_summary']}

âš¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸
- ChatGPT, Claude, Gemini ë“± ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ ì£¼ë„
- ë©€í‹°ëª¨ë‹¬ AIì™€ AI ì—ì´ì „íŠ¸ ê¸°ìˆ  ê¸‰ë¶€ìƒ
- AI ì•ˆì „ì„±ê³¼ ìœ¤ë¦¬ ë¬¸ì œ ì¤‘ìš”ì„± ì¦ëŒ€
- Edge AIì™€ ëª¨ë°”ì¼ AI ê¸°ìˆ  ë°œì „ ê°€ì†í™”
- AI ì½”ë”© ë„êµ¬ì˜ í”„ë¡œê·¸ë˜ë¨¸ ìƒì‚°ì„± í˜ì‹ 

ğŸš€ ê¶Œì¥ ì•¡ì…˜
1. ë©€í‹°ëª¨ë‹¬ AI ê¸°ìˆ  íˆ¬ì ë° ì—°êµ¬ ê°•í™”
2. AI ì•ˆì „ì„± ë° ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸ ìˆ˜ë¦½
3. Edge AI í™œìš© ëª¨ë°”ì¼ ì„œë¹„ìŠ¤ ê°œë°œ
4. AI ì½”ë”© ë„êµ¬ ë„ì…ìœ¼ë¡œ ê°œë°œ íš¨ìœ¨ì„± í–¥ìƒ

{'='*60}
ğŸ¨ Leonardo da Vinci 2075 Edition
AI íŠ¸ë Œë“œ ë¶„ì„ê¸° ë°ëª¨ ì™„ë£Œ!
"""
        
        return report
    
    async def run_demo(self):
        """ë°ëª¨ ì‹¤í–‰"""
        print("ğŸš€ AI íŠ¸ë Œë“œ ë¶„ì„ê¸° ë°ëª¨ ì‹œì‘!")
        print("ğŸ¨ Leonardo da Vinci 2075 Edition")
        print("="*60)
        
        try:
            # Phase 1: YouTube ë¶„ì„
            print("\nğŸ“º Phase 1: YouTube AI íŠ¸ë Œë“œ ë¶„ì„")
            youtube_data = await self.simulate_youtube_analysis()
            await asyncio.sleep(1)
            
            # Phase 2: ë¦¬ì„œì¹˜ ë¶„ì„
            print("\nğŸ” Phase 2: ê³ ê¸‰ ë¦¬ì„œì¹˜ ì†ŒìŠ¤ ë¶„ì„")
            research_data = await self.simulate_research_analysis()
            await asyncio.sleep(1)
            
            # Phase 3: ì¢…í•© ë¶„ì„
            print("\nğŸ“Š Phase 3: ì¢…í•© íŠ¸ë Œë“œ ë¶„ì„")
            analysis = await self.generate_trend_analysis(youtube_data, research_data)
            await asyncio.sleep(1)
            
            # Phase 4: ë¦¬í¬íŠ¸ ìƒì„±
            print("\nğŸ“‹ Phase 4: ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±")
            final_report = self.generate_final_report(analysis)
            
            # ë¦¬í¬íŠ¸ ì €ì¥
            report_file = f"/Users/Jadaking/DataLab/projects/ai_trend_analyzer/data/demo_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(final_report)
            
            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "="*60)
            print("ğŸ‰ ë°ëª¨ ì™„ë£Œ!")
            print(f"ğŸ“ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
            print("="*60)
            print(final_report)
            
            # ë°ì´í„°ë² ì´ìŠ¤ í˜„í™© ì¶œë ¥
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM youtube_videos")
            youtube_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM research_sources")
            research_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM trend_analysis")
            analysis_count = cursor.fetchone()[0]
            
            print(f"\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©:")
            print(f"   YouTube ì˜ìƒ: {youtube_count}ê°œ")
            print(f"   ë¦¬ì„œì¹˜ ì†ŒìŠ¤: {research_count}ê°œ")
            print(f"   íŠ¸ë Œë“œ ë¶„ì„: {analysis_count}ê°œ")
            
            cursor.close()
            conn.close()
            
            return {
                'success': True,
                'report_file': report_file,
                'analysis': analysis
            }
            
        except Exception as e:
            print(f"âŒ ë°ëª¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    demo = AITrendDemo()
    result = await demo.run_demo()
    
    if result['success']:
        print("\nğŸ¯ ë°ëª¨ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        print("ğŸ¨ Leonardo da Vincië„ ë§Œì¡±í•  ë¶„ì„ì´ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"\nğŸ˜ ë°ëª¨ ì‹¤í–‰ ì‹¤íŒ¨: {result['error']}")

if __name__ == "__main__":
    asyncio.run(main())
